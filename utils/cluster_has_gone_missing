#!/bin/bash

set -o xtrace
set -e
set -o nounset
set -o pipefail

usage() {
    cat <<EOF
    usage: $0 [ OPTION ] CLUSTER

    CLUSTER    The name or ID of the cluster
EOF
}

if [ -z "${1}" ]; then
    echo ""
    echo "    $0 requires an argument!"
    usage
    exit 1
fi

CLUSTER=$1

CLUSTER_COUNT=$(ocm list cluster --no-headers $CLUSTER | wc -l)
if [[ $CLUSTER_COUNT > 1 ]]; then
    echo -e "wrong number of clusters found: ${CLUSTER_COUNT}\nexiting..."
    exit
fi

TMPD=$(mktemp -d)
# If you're using `backplane tunnel -D`, just `export BACKPLANE_TUNNEL_ALREADY_UP=yes` in your .bashrc
# and this script will just assume the tunnels are up
TUNNEL_ALREADY_UP=${BACKPLANE_TUNNEL_ALREADY_UP:-dunno}
LOGGED_IN=false

cleanup() {
    rm -rf $TMPD
    if $LOGGED_IN ; then
        ocm backplane logout > /dev/null
        [ $TUNNEL_ALREADY_UP != "yes" ] && pkill -f "$(which ocm-backplane) tunnel ${SHARD}"
    fi
}
trap cleanup EXIT

echo -e "getting cluster info for ${CLUSTER}...\n"
ocm describe cluster $CLUSTER > $TMPD/clusterinfo
if [ $? != 0 ]; then
    exit
fi

read CLUSTER_INT_ID CLUSTER_EXT_ID CLUSTER_NAME CLUSTER_REGION CLOUD_PROVIDER < <(echo $(ocm describe cluster $CLUSTER --json | jq -r '.id,.external_id,.name,.region.id,.cloud_provider.id'))
# SHARD not output from ocm describe cluster --json https://github.com/openshift-online/ocm-cli/issues/363
SHARD=$(awk -F. '$1 ~ /^Shard:/ {print $2}' $TMPD/clusterinfo  | sed 's/^ *//')

if [ -f "/tmp/sshuttle_$SHARD.pid" ]; then
    ps -p $(cat "/tmp/sshuttle_$SHARD.pid") >/dev/null 2>&1
    [ $? = 0 ] && TUNNEL_ALREADY_UP=yes
fi

if [ $TUNNEL_ALREADY_UP != "yes" ]; then
    # frob sudo. when `ocm backplane tunnel` is backgrounded, it will hang
    # indefinitely if sshuttle has to wait for sudo
    test ${UID} -eq 0 || sudo -v
    ocm backplane tunnel $SHARD > /dev/null 2>&1 &
fi
ocm backplane login $SHARD > /dev/null
LOGGED_IN=true

if [ $CLOUD_PROVIDER == "gcp" ]; then
    GCP_PROJECT_ID=$(oc get --no-headers projectclaim -n uhc-production-$CLUSTER_INT_ID | awk '{print $3}')
    echo -e "Cluster is on GCP, please check VM status manually here: https://console.cloud.google.com/home/dashboard?project=${GCP_PROJECT_ID}"
    exit
fi

# get temporary creds
# export AWS_PROFILE=rhcontrol # CHGM isn't a concern in stage or int
# if `osdctl account cli` fails to sts:AssumeRole, don't exit this script, post a service log message instead
set +e
CREDS=$(ocm backplane cloud credentials $CLUSTER_INT_ID -o json 2>&1 | grep -v Throttling)
assume_role_error_pattern="RH-SRE-.* is not authorized to perform: sts:AssumeRole"
if [[ ${CREDS} =~ ${assume_role_error_pattern} ]]; then
    echo -e "cannot assume role\nsending service log notice..."
    # osdctl servicelog post ${CLUSTER_EXT_ID} -t https://raw.githubusercontent.com/openshift/managed-notifications/master/osd/invalid_iaas_credentials.json
    exit
fi

backplane_tunnel_error="unable to connect to backplane api, please check if the tunnel is running"
if [[ ${CREDS} =~ ${backplane_tunnel_error} ]] ; then
  echo -e "${backplane_tunnel_error}"
  exit
fi
set -e
export AWS_ACCESS_KEY_ID=$(echo "${CREDS}" | jq -r '.AccessKeyId')
export AWS_SECRET_ACCESS_KEY=$(echo "${CREDS}" | jq -r '.SecretAccessKey')
export AWS_SESSION_TOKEN=$(echo "${CREDS}" | jq -r '.SessionToken')
export AWS_DEFAULT_REGION="${CLUSTER_REGION}"

# find the cluster's ownership tag and use that to find the master nodes
# filter names can't include wildcards, so we have to find the tag with describe-tags
CLUSTER_TAG=$(aws ec2 describe-tags --filter Name=key,Values=kubernetes.io/cluster/${CLUSTER_NAME}-* Name=resource-type,Values=instance | jq -r '.Tags[0].Key')
echo "CLUSTER_TAG=${CLUSTER_TAG}"

INSTANCES_JSON=$(aws ec2 describe-instances --filter Name=tag:${CLUSTER_TAG},Values=owned Name=tag:Name,Values=*master-* | jq -r '.Reservations[].Instances[]')
echo "INSTANCES_JSON=${INSTANCES_JSON}"

INSTANCE_IDS=$(echo "${INSTANCES_JSON}" | jq -r '.InstanceId')
echo "INSTANCE_IDS=${INSTANCE_IDS}"

# check if the masters are stopped / terminated
ANY_RUNNING=false
for INSTANCE_ID in $INSTANCE_IDS; do
    INSTANCE_STATE=$(echo "${INSTANCES_JSON}" | jq -r --arg instance_id "${INSTANCE_ID}" 'select(.InstanceId == $instance_id) | .State.Name')
    if [[ "${INSTANCE_STATE}" == "running" ]]; then
      ANY_RUNNING=true
    fi
done

if $ANY_RUNNING ; then
    echo -e "not all masters stopped, please investigate\nexiting\n\nMasters:"
    echo "$INSTANCES_JSON" | jq -rj '.InstanceId, " ", .State.Name, "\n"'
    CONSOLE_OUTPUT=$(ocm backplane cloud console $CLUSTER_INT_ID -o json 2>&1 | grep -v Throttling)
    echo -e "\n${CONSOLE_OUTPUT}"
    exit
fi

# if the instance stopped or terminated, look for stop / terminate events for
# the instance in cloudtrail
EVENTS=$(aws cloudtrail lookup-events --lookup-attributes AttributeKey=ResourceName,AttributeValue=${INSTANCE_ID})
# who did it?
STOPPED_BY=$(echo "${EVENTS}" | jq -r '.Events[] | select(.EventName == "StopInstances" or .EventName == "TerminateInstances") | .Username' | head -n 1)

# if the customer, send servicelog
if [[ ! "${STOPPED_BY}" =~ ^RH-SRE- ]]; then
    echo -e "instances stopped by ${STOPPED_BY}, not SRE\nsending service log notice..."
    osdctl servicelog post ${CLUSTER_EXT_ID} -t https://raw.githubusercontent.com/openshift/managed-notifications/master/osd/cluster_has_gone_missing.json
    exit
else
    echo -e "instances stopped by SRE\nplease investigate!"
fi

